{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling \n",
    "\n",
    "we will be covering the steps on how to do Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaahhh</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aah</th>\n",
       "      <th>aall</th>\n",
       "      <th>aand</th>\n",
       "      <th>aarrives</th>\n",
       "      <th>aaw</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yup</th>\n",
       "      <th>yyou</th>\n",
       "      <th>yyour</th>\n",
       "      <th>zabadaladabaabala</th>\n",
       "      <th>zero</th>\n",
       "      <th>zipup</th>\n",
       "      <th>zubadabadabaabala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aaaah  aaahhh  aaaw  aah  aall  aand  aarrives  aaw  abandon  \\\n",
       "Amanda Seales       0       0     0    0     0     0         0    0        0   \n",
       "Bert Kreischer      0       0     0    0     0     0         0    0        0   \n",
       "Marc Maron          0       0     0    0     0     0         0    0        0   \n",
       "Pete Davidson       0       0     0    0     1     3         1    0        0   \n",
       "Stewart Lee         1       1     1    1     0     0         0    2        1   \n",
       "\n",
       "                abbey  ...  youngest  youtube  youve  yup  yyou  yyour  \\\n",
       "Amanda Seales       0  ...         0        0      1    0     0      0   \n",
       "Bert Kreischer      1  ...         2        0      2    0     0      0   \n",
       "Marc Maron          0  ...         0        0      2    3     0      0   \n",
       "Pete Davidson       0  ...         0        0      1    0     2      1   \n",
       "Stewart Lee         0  ...         0        1     20    0     0      0   \n",
       "\n",
       "                zabadaladabaabala  zero  zipup  zubadabadabaabala  \n",
       "Amanda Seales                   1     1      0                  1  \n",
       "Bert Kreischer                  0     0      0                  0  \n",
       "Marc Maron                      0     0      0                  0  \n",
       "Pete Davidson                   0     0      0                  0  \n",
       "Stewart Lee                     0     0      1                  0  \n",
       "\n",
       "[5 rows x 4532 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amanda Seales</th>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <th>Marc Maron</th>\n",
       "      <th>Pete Davidson</th>\n",
       "      <th>Stewart Lee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaahhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aall</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Amanda Seales  Bert Kreischer  Marc Maron  Pete Davidson  Stewart Lee\n",
       "aaaah               0               0           0              0            1\n",
       "aaahhh              0               0           0              0            1\n",
       "aaaw                0               0           0              0            1\n",
       "aah                 0               0           0              0            1\n",
       "aall                0               0           0              1            0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"goes\" + 0.006*\"gonna\" + 0.006*\"oh\" + 0.006*\"man\" + 0.006*\"hes\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"did\" + 0.005*\"okay\" + 0.005*\"theyre\"'),\n",
       " (1,\n",
       "  '0.015*\"uh\" + 0.009*\"people\" + 0.006*\"said\" + 0.006*\"theres\" + 0.006*\"going\" + 0.006*\"ive\" + 0.005*\"laughter\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"hes\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"uh\" + 0.008*\"goes\" + 0.007*\"cause\" + 0.006*\"oh\" + 0.006*\"hes\" + 0.006*\"fuck\" + 0.006*\"gonna\" + 0.006*\"guy\" + 0.006*\"man\" + 0.006*\"people\"'),\n",
       " (1,\n",
       "  '0.000*\"people\" + 0.000*\"uh\" + 0.000*\"goes\" + 0.000*\"hes\" + 0.000*\"oh\" + 0.000*\"fuck\" + 0.000*\"said\" + 0.000*\"theres\" + 0.000*\"going\" + 0.000*\"man\"'),\n",
       " (2,\n",
       "  '0.011*\"people\" + 0.008*\"laughter\" + 0.008*\"going\" + 0.007*\"said\" + 0.007*\"mate\" + 0.007*\"theres\" + 0.006*\"ive\" + 0.005*\"joke\" + 0.005*\"sort\" + 0.005*\"hes\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"uh\" + 0.009*\"cause\" + 0.008*\"people\" + 0.007*\"guy\" + 0.007*\"okay\" + 0.007*\"fuck\" + 0.007*\"oh\" + 0.007*\"man\" + 0.006*\"gonna\" + 0.006*\"really\"'),\n",
       " (1,\n",
       "  '0.018*\"goes\" + 0.008*\"dad\" + 0.008*\"hes\" + 0.007*\"shes\" + 0.006*\"did\" + 0.006*\"want\" + 0.006*\"gonna\" + 0.006*\"ila\" + 0.005*\"gun\" + 0.005*\"day\"'),\n",
       " (2,\n",
       "  '0.012*\"people\" + 0.009*\"laughter\" + 0.009*\"going\" + 0.008*\"said\" + 0.007*\"mate\" + 0.007*\"theres\" + 0.007*\"ive\" + 0.006*\"joke\" + 0.005*\"sort\" + 0.005*\"hes\"'),\n",
       " (3,\n",
       "  '0.001*\"uh\" + 0.000*\"cause\" + 0.000*\"guy\" + 0.000*\"theres\" + 0.000*\"okay\" + 0.000*\"said\" + 0.000*\"did\" + 0.000*\"man\" + 0.000*\"people\" + 0.000*\"oh\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well.\n",
    "\n",
    "## Topic Modeling - Attempt #2 (Nouns Only)\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Romin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>now yall keep asking me amanda who is this spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>ladies and gentlemen bert kreischer  yeah wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>ladies and gentlemen please welcome to the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>so louis ck tried to get me fired from snl my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>this programme contains very strong language a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       transcript\n",
       "Amanda Seales   now yall keep asking me amanda who is this spe...\n",
       "Bert Kreischer    ladies and gentlemen bert kreischer  yeah wh...\n",
       "Marc Maron        ladies and gentlemen please welcome to the s...\n",
       "Pete Davidson   so louis ck tried to get me fired from snl my ...\n",
       "Stewart Lee     this programme contains very strong language a..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Romin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>amanda yall sisters comedy everybody everybody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>ladies gentlemen yeah cleveland i shit bathroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>ladies gentlemen stage marc maron thank thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>ck snl year story like uh finale snl ii i get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>programme language humour floorboards id junct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       transcript\n",
       "Amanda Seales   amanda yall sisters comedy everybody everybody...\n",
       "Bert Kreischer  ladies gentlemen yeah cleveland i shit bathroo...\n",
       "Marc Maron      ladies gentlemen stage marc maron thank thanks...\n",
       "Pete Davidson   ck snl year story like uh finale snl ii i get ...\n",
       "Stewart Lee     programme language humour floorboards id junct..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaw</th>\n",
       "      <th>abbey</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abusers</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yyou</th>\n",
       "      <th>zabadaladabaabala</th>\n",
       "      <th>zero</th>\n",
       "      <th>zipup</th>\n",
       "      <th>zubadabadabaabala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2695 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aaaah  aaaw  aand  aaw  abbey  aberdeen  abortion  abusers  \\\n",
       "Amanda Seales       0     0     0    0      0         0         0        1   \n",
       "Bert Kreischer      0     0     0    0      1         0         1        0   \n",
       "Marc Maron          0     0     0    0      0         0         0        0   \n",
       "Pete Davidson       0     0     1    0      0         0         0        0   \n",
       "Stewart Lee         1     1     0    2      0         1         0        0   \n",
       "\n",
       "                access  accident  ...  york  youd  youll  youtube  youve  \\\n",
       "Amanda Seales        2         0  ...     1     0      1        0      0   \n",
       "Bert Kreischer       0         0  ...     0     1      2        0      1   \n",
       "Marc Maron           0         1  ...     0     1      0        0      1   \n",
       "Pete Davidson        0         1  ...     0     0      0        0      1   \n",
       "Stewart Lee          2         1  ...     1     4      1        1      7   \n",
       "\n",
       "                yyou  zabadaladabaabala  zero  zipup  zubadabadabaabala  \n",
       "Amanda Seales      0                  1     1      0                  1  \n",
       "Bert Kreischer     0                  0     0      0                  0  \n",
       "Marc Maron         0                  0     0      0                  0  \n",
       "Pete Davidson      1                  0     0      0                  0  \n",
       "Stewart Lee        0                  0     0      1                  0  \n",
       "\n",
       "[5 rows x 2695 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"man\" + 0.010*\"jesus\" + 0.010*\"day\" + 0.008*\"gon\" + 0.008*\"guy\" + 0.007*\"lot\" + 0.006*\"cause\" + 0.006*\"hes\" + 0.006*\"sort\" + 0.006*\"thing\"'),\n",
       " (1,\n",
       "  '0.010*\"hes\" + 0.009*\"uh\" + 0.009*\"thing\" + 0.007*\"guy\" + 0.006*\"joke\" + 0.006*\"theres\" + 0.006*\"night\" + 0.006*\"dad\" + 0.006*\"man\" + 0.005*\"cause\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"man\" + 0.001*\"hes\" + 0.001*\"day\" + 0.000*\"sort\" + 0.000*\"thing\" + 0.000*\"lot\" + 0.000*\"night\" + 0.000*\"joke\" + 0.000*\"jesus\" + 0.000*\"gon\"'),\n",
       " (1,\n",
       "  '0.013*\"man\" + 0.012*\"day\" + 0.011*\"hes\" + 0.009*\"gon\" + 0.008*\"guy\" + 0.008*\"jesus\" + 0.008*\"shes\" + 0.007*\"lot\" + 0.007*\"cause\" + 0.007*\"fuck\"'),\n",
       " (2,\n",
       "  '0.012*\"uh\" + 0.010*\"thing\" + 0.008*\"hes\" + 0.008*\"joke\" + 0.007*\"theres\" + 0.007*\"guy\" + 0.007*\"mate\" + 0.007*\"laughter\" + 0.007*\"sort\" + 0.005*\"night\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"mate\" + 0.010*\"laughter\" + 0.010*\"joke\" + 0.009*\"sort\" + 0.009*\"thing\" + 0.008*\"theres\" + 0.007*\"game\" + 0.007*\"night\" + 0.006*\"hes\" + 0.006*\"thrones\"'),\n",
       " (1,\n",
       "  '0.016*\"uh\" + 0.014*\"hes\" + 0.013*\"guy\" + 0.011*\"dad\" + 0.010*\"cause\" + 0.009*\"man\" + 0.008*\"thing\" + 0.008*\"day\" + 0.008*\"shes\" + 0.008*\"fuck\"'),\n",
       " (2,\n",
       "  '0.011*\"cause\" + 0.009*\"women\" + 0.008*\"okay\" + 0.008*\"aint\" + 0.007*\"man\" + 0.006*\"gon\" + 0.006*\"yall\" + 0.006*\"shit\" + 0.006*\"dick\" + 0.006*\"woman\"'),\n",
       " (3,\n",
       "  '0.017*\"man\" + 0.017*\"jesus\" + 0.014*\"day\" + 0.013*\"guy\" + 0.012*\"lot\" + 0.010*\"sort\" + 0.009*\"gon\" + 0.009*\"hes\" + 0.008*\"way\" + 0.008*\"thing\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>amanda special i yall sisters comedy everybody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>ladies gentlemen kreischer yeah cleveland i sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>ladies gentlemen welcome stage marc maron than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>louis ck snl first year story like uh finale s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>programme strong language humour floorboards i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       transcript\n",
       "Amanda Seales   amanda special i yall sisters comedy everybody...\n",
       "Bert Kreischer  ladies gentlemen kreischer yeah cleveland i sh...\n",
       "Marc Maron      ladies gentlemen welcome stage marc maron than...\n",
       "Pete Davidson   louis ck snl first year story like uh finale s...\n",
       "Stewart Lee     programme strong language humour floorboards i..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaahhh</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aall</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaw</th>\n",
       "      <th>abbey</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yyou</th>\n",
       "      <th>zabadaladabaabala</th>\n",
       "      <th>zero</th>\n",
       "      <th>zipup</th>\n",
       "      <th>zubadabadabaabala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amanda Seales</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert Kreischer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marc Maron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pete Davidson</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart Lee</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aaaah  aaahhh  aaaw  aall  aand  aaw  abbey  aberdeen  able  \\\n",
       "Amanda Seales       0       0     0     0     0    0      0         0     0   \n",
       "Bert Kreischer      0       0     0     0     0    0      1         0     1   \n",
       "Marc Maron          0       0     0     0     0    0      0         0     1   \n",
       "Pete Davidson       0       0     0     1     1    0      0         0     1   \n",
       "Stewart Lee         1       1     1     0     0    2      0         1     2   \n",
       "\n",
       "                abortion  ...  youll  young  younger  youngest  youtube  yyou  \\\n",
       "Amanda Seales          0  ...      1      2        0         0        0     0   \n",
       "Bert Kreischer         1  ...      2      1        0         2        0     0   \n",
       "Marc Maron             0  ...      0      0        1         0        0     0   \n",
       "Pete Davidson          0  ...      0      0        0         0        0     1   \n",
       "Stewart Lee            0  ...      2      7        0         0        1     0   \n",
       "\n",
       "                zabadaladabaabala  zero  zipup  zubadabadabaabala  \n",
       "Amanda Seales                   1     1      0                  1  \n",
       "Bert Kreischer                  0     0      0                  0  \n",
       "Marc Maron                      0     0      0                  0  \n",
       "Pete Davidson                   0     0      0                  0  \n",
       "Stewart Lee                     0     0      1                  0  \n",
       "\n",
       "[5 rows x 3325 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"sort\" + 0.006*\"jesus\" + 0.005*\"laughter\" + 0.005*\"mate\" + 0.005*\"lot\" + 0.005*\"black\" + 0.004*\"white\" + 0.004*\"gon\" + 0.004*\"guy\" + 0.004*\"kind\"'),\n",
       " (1,\n",
       "  '0.011*\"guy\" + 0.010*\"dad\" + 0.008*\"cause\" + 0.006*\"hey\" + 0.006*\"fuckin\" + 0.005*\"wife\" + 0.005*\"gun\" + 0.005*\"gon\" + 0.005*\"dick\" + 0.004*\"lot\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"gon\" + 0.008*\"guy\" + 0.008*\"jesus\" + 0.008*\"black\" + 0.006*\"lot\" + 0.006*\"cause\" + 0.006*\"white\" + 0.006*\"okay\" + 0.005*\"dad\" + 0.005*\"dick\"'),\n",
       " (1,\n",
       "  '0.009*\"mate\" + 0.009*\"laughter\" + 0.008*\"sort\" + 0.006*\"game\" + 0.005*\"thrones\" + 0.005*\"sex\" + 0.004*\"standup\" + 0.004*\"brexit\" + 0.004*\"internet\" + 0.004*\"southend\"'),\n",
       " (2,\n",
       "  '0.015*\"guy\" + 0.011*\"cause\" + 0.011*\"fuckin\" + 0.008*\"dad\" + 0.007*\"sorry\" + 0.007*\"eye\" + 0.006*\"gay\" + 0.005*\"dick\" + 0.005*\"lot\" + 0.005*\"hey\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"white\" + 0.016*\"black\" + 0.010*\"okay\" + 0.009*\"cause\" + 0.007*\"delete\" + 0.006*\"aint\" + 0.006*\"gon\" + 0.006*\"yall\" + 0.005*\"harriet\" + 0.005*\"dick\"'),\n",
       " (1,\n",
       "  '0.010*\"sort\" + 0.007*\"jesus\" + 0.007*\"laughter\" + 0.007*\"mate\" + 0.006*\"lot\" + 0.005*\"guy\" + 0.005*\"kind\" + 0.004*\"game\" + 0.004*\"years\" + 0.004*\"sex\"'),\n",
       " (2,\n",
       "  '0.001*\"mate\" + 0.001*\"game\" + 0.000*\"laughter\" + 0.000*\"thrones\" + 0.000*\"sort\" + 0.000*\"sex\" + 0.000*\"guy\" + 0.000*\"years\" + 0.000*\"black\" + 0.000*\"gon\"'),\n",
       " (3,\n",
       "  '0.013*\"guy\" + 0.013*\"dad\" + 0.010*\"cause\" + 0.007*\"hey\" + 0.007*\"fuckin\" + 0.006*\"gun\" + 0.006*\"wife\" + 0.006*\"gon\" + 0.006*\"dick\" + 0.005*\"lot\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document\n",
    "\n",
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"wall\" + 0.000*\"wind\" + 0.000*\"frozen\" + 0.000*\"emails\" + 0.000*\"harry\" + 0.000*\"road\" + 0.000*\"racist\" + 0.000*\"sir\" + 0.000*\"sentence\" + 0.000*\"drop\"'),\n",
       " (1,\n",
       "  '0.012*\"dad\" + 0.009*\"gun\" + 0.009*\"wife\" + 0.008*\"ila\" + 0.007*\"guns\" + 0.007*\"hey\" + 0.006*\"phone\" + 0.006*\"kid\" + 0.006*\"georgia\" + 0.005*\"period\"'),\n",
       " (2,\n",
       "  '0.010*\"laughter\" + 0.010*\"mate\" + 0.008*\"sort\" + 0.006*\"game\" + 0.006*\"thrones\" + 0.005*\"sex\" + 0.005*\"standup\" + 0.005*\"brexit\" + 0.004*\"internet\" + 0.004*\"southend\"'),\n",
       " (3,\n",
       "  '0.014*\"guy\" + 0.010*\"cause\" + 0.009*\"jesus\" + 0.009*\"gon\" + 0.008*\"lot\" + 0.007*\"okay\" + 0.006*\"white\" + 0.006*\"dick\" + 0.006*\"black\" + 0.005*\"sorry\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'Amanda Seales'),\n",
       " (1, 'Bert Kreischer'),\n",
       " (3, 'Marc Maron'),\n",
       " (3, 'Pete Davidson'),\n",
       " (2, 'Stewart Lee')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
